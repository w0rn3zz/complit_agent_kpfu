# train_simple.py
import pandas as pd
import numpy as np
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report
import joblib
import torch
from transformers import AutoTokenizer, AutoModel
from collections import Counter

# ---------- 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö ----------
df = pd.read_csv('new_output_dataset.csv')
df = df.dropna(subset=['–¢–µ–∫—Å—Ç –∑–∞—è–≤–∫–∏', '–ö–ª–∞—Å—Å'])
texts = df['–¢–µ–∫—Å—Ç –∑–∞—è–≤–∫–∏'].astype(str).tolist()
labels = df['–ö–ª–∞—Å—Å'].tolist()

# –ü–æ–¥—Å—á–∏—Ç—ã–≤–∞–µ–º —á–∞—Å—Ç–æ—Ç—ã
label_counts = Counter(labels)
print("–ö–ª–∞—Å—Å—ã —Å 1 –ø—Ä–∏–º–µ—Ä–æ–º:", [k for k, v in label_counts.items() if v == 1])

# –û—Å—Ç–∞–≤–ª—è–µ–º —Ç–æ–ª—å–∫–æ –∫–ª–∞—Å—Å—ã —Å ‚â•2 –ø—Ä–∏–º–µ—Ä–∞–º–∏
valid_labels = {k for k, v in label_counts.items() if v >= 2}
mask = [label in valid_labels for label in labels]
texts = [t for t, m in zip(texts, mask) if m]
labels = [l for l, m in zip(labels, mask) if m]

print(f"–û—Å—Ç–∞–≤–ª–µ–Ω–æ {len(labels)} –ø—Ä–∏–º–µ—Ä–æ–≤ –ø–æ—Å–ª–µ —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ —Ä–µ–¥–∫–∏—Ö –∫–ª–∞—Å—Å–æ–≤.")

# ---------- 2. –≠–º–±–µ–¥–¥–∏–Ω–≥–∏ —á–µ—Ä–µ–∑ rubert-tiny2 ----------
print("–ó–∞–≥—Ä—É–∂–∞–µ–º rubert-tiny2...")
tokenizer = AutoTokenizer.from_pretrained("./rubert-tiny2-local")
model = AutoModel.from_pretrained("./rubert-tiny2-local")

def get_embedding(text):
    inputs = tokenizer(text, return_tensors="pt", truncation=True, padding=True, max_length=256)
    with torch.no_grad():
        outputs = model(**inputs)
    # –ë–µ—Ä—ë–º [CLS] —Ç–æ–∫–µ–Ω –∏ –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ–º
    cls_embedding = outputs.last_hidden_state[:, 0, :].numpy()
    return cls_embedding[0]

print("–ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏...")
embeddings = np.array([get_embedding(text) for text in texts])

# ---------- 3. –û–±—É—á–µ–Ω–∏–µ –∫–ª–∞—Å—Å–∏—Ñ–∏–∫–∞—Ç–æ—Ä–∞ ----------
X_train, X_test, y_train, y_test = train_test_split(
    embeddings, labels, test_size=0.2, random_state=42, stratify=labels
)

print("–û–±—É—á–∞–µ–º –ª–æ–≥–∏—Å—Ç–∏—á–µ—Å–∫—É—é —Ä–µ–≥—Ä–µ—Å—Å–∏—é...")
clf = LogisticRegression(
    max_iter=1000,
    random_state=42,
    class_weight='balanced',
    C=10.0  # —Ä–µ–≥—É–ª—è—Ä–∏–∑–∞—Ü–∏—è
)
clf.fit(X_train, y_train)

# ---------- 4. –û—Ü–µ–Ω–∫–∞ ----------
y_pred = clf.predict(X_test)
print("\nüìä –¢–æ—á–Ω–æ—Å—Ç—å:")
print(classification_report(y_test, y_pred, zero_division=0))

# ---------- 5. –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ ----------
joblib.dump(clf, 'logistic_classifier_new_dataset.pkl')
joblib.dump(tokenizer, 'tokenizer_new_dataset.pkl')
# –ú–æ–¥–µ–ª—å BERT —É–∂–µ –ª–æ–∫–∞–ª—å–Ω–æ ‚Äî –Ω–µ –Ω—É–∂–Ω–æ —Å–æ—Ö—Ä–∞–Ω—è—Ç—å –∑–∞–Ω–æ–≤–æ

print("\n‚úÖ –ú–æ–¥–µ–ª—å —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞: logistic_classifier_new_dataset.pkl")